import os
import sys
import logging
from datetime import datetime
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.tools import Tool
from langchain_community.utilities import GoogleSerperAPIWrapper
from langchain.agents import initialize_agent, AgentType
from dotenv import load_dotenv

# é—œé–‰ LangSmith è¿½è¹¤
os.environ["LANGCHAIN_TRACING_V2"] = "false"
os.environ["LANGCHAIN_ENDPOINT"] = ""
os.environ["LANGCHAIN_API_KEY"] = ""
os.environ["LANGCHAIN_PROJECT"] = ""

# è¨­å®šè©³ç´° log æª”æ¡ˆ
log_filename = f"agent_debug_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_filename, encoding='utf-8'),
        logging.StreamHandler()  # åŒæ™‚é¡¯ç¤ºåœ¨çµ‚ç«¯æ©Ÿ
    ]
)

# è®€å– .env
load_dotenv()

# 1. RAG è¨­å®š
embedding_model = HuggingFaceEmbeddings(
    model_name="BAAI/bge-m3",
    model_kwargs={"device": "cpu"}
)
vectorstore = Chroma(
    persist_directory="chroma_db",
    collection_name="doctorv4",
    embedding_function=embedding_model
)
retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
llm = ChatOpenAI(model="gpt-4o", temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever
)

def rag_query(query: str) -> str:
    logging.info(f"ğŸ” RAG æª¢ç´¢é–‹å§‹: {query}")
    logging.info(f"ğŸ“Š å‘é‡è³‡æ–™åº«æŸ¥è©¢: {query}")
    docs = vectorstore.similarity_search(query, k=3)
    logging.info(f" æ‰¾åˆ° {len(docs)} å€‹ç›¸é—œæ–‡ä»¶")
    for i, doc in enumerate(docs):
        logging.info(f"   æ–‡ä»¶ {i+1}: {doc.page_content[:100]}...")
    
    result = qa_chain.invoke({"query": query})
    response = result['result'] if isinstance(result, dict) else str(result)
    logging.info(f"ğŸ¤– RAG æœ€çµ‚çµæœ: {response[:200]}...")
    return response

rag_tool = Tool(
    name="é†«å¸«è³‡æ–™åº«æŸ¥è©¢",
    func=rag_query,
    description="æŸ¥è©¢æœ¬åœ°é†«å¸«è³‡æ–™åº«ï¼ŒåŒ…å«é†«å¸«å§“åã€å°ˆé•·ã€å­¸æ­·ã€ç¶“æ­·ã€è·ç¨±ç­‰è³‡è¨Šã€‚"
)

# 2. Web Search Tool (Serper only)
serper_wrapper = GoogleSerperAPIWrapper()
def smart_web_search(query: str) -> str:
    logging.info(f"ğŸŒ Web Search é–‹å§‹: {query}")
    try:
        result = serper_wrapper.run(query)
        logging.info(f"ğŸŒ Web Search çµæœ: {result[:200]}...")
        return result
    except Exception as e:
        error_msg = f"Web search failed: {e}"
        logging.error(f"âŒ Web Search éŒ¯èª¤: {error_msg}")
        return error_msg

web_search_tool = Tool(
    name="ç¶²è·¯æœå°‹",
    func=smart_web_search,
    description="æŸ¥è©¢ç¶²è·¯æœ€æ–°è³‡è¨Šï¼Œé©åˆæŸ¥è©¢é†«ç™‚æ–°çŸ¥ã€æœ€æ–°æ²»ç™‚æ–¹æ³•ã€ç ”ç©¶å ±å‘Šç­‰ã€‚"
)

# 3. Agent
# å·¥å…·é †åºï¼šRAGã€Web Search
tools = [rag_tool, web_search_tool]
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True,
    handle_parsing_errors=True,
    max_iterations=5,
    early_stopping_method="generate"
)

# 4. æ¸¬è©¦
if __name__ == "__main__":
    logging.info("ğŸ§ª æ¸¬è©¦é†«ç™‚è³‡è¨ŠæŸ¥è©¢ç³»çµ±")
    logging.info("=" * 60)
    
    # å¯ä»¥ä¿®æ”¹é€™è£¡çš„å•é¡Œä¾†æ¸¬è©¦ä¸åŒæŸ¥è©¢
    question = "æœ±å¿—ç”Ÿé†«å¸«å°ˆé•·"
    logging.info(f"å•é¡Œ: {question}")
    logging.info("=" * 60)
    
    result = agent.run(question)
    
    logging.info("=" * 60)
    logging.info(f"æœ€çµ‚çµæœ: {result}")
    logging.info(f"ğŸ“ è©³ç´° log æª”æ¡ˆ: {log_filename}") 